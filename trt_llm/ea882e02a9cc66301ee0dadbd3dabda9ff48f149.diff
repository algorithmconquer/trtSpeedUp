diff --git a/examples/flux/README.md b/examples/flux/README.md
index 3c51d767099916040a33b59b8103b2a14c6da4c3..ecd686554e9f873a278478e02cccbe1ccee0c819 100644
--- a/examples/flux/README.md
+++ b/examples/flux/README.md
@@ -27,7 +27,7 @@ This checkpoint will be converted to the TensorRT-LLM checkpoint format by [`con
 
 ```
 # Convert to TRT-LLM
-python convert_checkpoint.py
+python convert_checkpoint.py --model_dir ./FLUX.1-dev
 trtllm-build --checkpoint_dir ./tllm_checkpoint/ \
                 --max_batch_size 1 \
                 --remove_input_padding disable
@@ -49,8 +49,8 @@ Just run `python run.py` and we can see an image named `flux-dev.png` will be ge
 We can levaerage tensor parallel to further reduce latency and memory consumption on each GPU. We take 4 GPUs parallelism as an example:
 
 ```
-# build dit engine
-python convert_checkpoint.py --tp_size 4
+# build engine
+python convert_checkpoint.py --model_dir ./FLUX.1-dev --tp_size 4
 trtllm-build --checkpoint_dir ./tllm_checkpoint/ \
                 --max_batch_size 1 \
                 --remove_input_padding disable
@@ -63,10 +63,11 @@ mpirun -n 4 --allow-run-as-root python run.py
 Context parallel is well at reduce latency since it has lower communication cost.
 
 ```
-# build dit engine
-python convert_checkpoint.py --cp_size 4
+# build engine
+python convert_checkpoint.py --model_dir ./FLUX.1-dev --cp_size 4
 trtllm-build --checkpoint_dir ./tllm_checkpoint/ \
                 --max_batch_size 1 \
+                --bert_attention_plugin disable \
                 --remove_input_padding disable
 # run
 mpirun -n 4 --allow-run-as-root python run.py
@@ -77,7 +78,7 @@ mpirun -n 4 --allow-run-as-root python run.py
 Tensor Parallel and Context Parallel can be used together to better balance latency and memory consumption.
 
 ```
-# build dit engine
+# build engine
 python convert_checkpoint.py --cp_size 2 --tp_size 2
 trtllm-build --checkpoint_dir ./tllm_checkpoint/ \
                 --max_batch_size 1 \
diff --git a/tensorrt_llm/models/flux/model.py b/tensorrt_llm/models/flux/model.py
index c3da490b4b5e1372746bf111e6e13dc872936da9..b02ecb1c3b413b0ac153625201106c2865c8bbb9 100644
--- a/tensorrt_llm/models/flux/model.py
+++ b/tensorrt_llm/models/flux/model.py
@@ -534,13 +534,12 @@ class FluxTransformer2DModel(PretrainedModel):
         if img_ids.ndim() == 3:
             img_ids = img_ids[0]
 
+        if self.mapping.cp_size > 1:
+            txt_ids = chunk(txt_ids, chunks=self.mapping.cp_size, dim=0)[self.mapping.cp_rank]
+            img_ids = chunk(img_ids, chunks=self.mapping.cp_size, dim=0)[self.mapping.cp_rank]
+
         ids = concat((txt_ids, img_ids), dim=0)
         image_rotary_emb = self.pos_embed(ids)
-        if self.mapping.cp_size > 1:
-            cos, sin = image_rotary_emb
-            cos = chunk(cos, chunks=self.mapping.cp_size, dim=0)[self.mapping.cp_rank]
-            sin = chunk(sin, chunks=self.mapping.cp_size, dim=0)[self.mapping.cp_rank]
-            image_rotary_emb = (cos, sin)
 
         for index_block, block in enumerate(self.transformer_blocks):
             encoder_hidden_states, hidden_states = block(
